# Zero
Dynamic Function Net

泛函分析？
解决神经网络在外延推断上的问题：
例如sin(x)的多项式展开不收敛，如果让神经网络基于多项式的函数结构学习sin(x)，那么一定无法处理外延问题
当前神经网络普遍存在这样的问题，在内插空间上表现良好，但在外延上表现通常很糟糕
这很可能源于函数表示结构的缺陷

无论是conv或linear层的kx+b，还是Attention的ax^2+bx+c
都是在学习目标函数的多项式展开，当目标函数的多项式展开无法收敛时，神经网络仅能学到内插的部分，一旦数据到达外延区就会偏离预期

正确的函数表示结构是良好泛化能力的关键

虽然sin(x)在多项式展开不收敛，但其在傅里叶展开下收敛
为函数添加不同的表示结构是可能的解决方案，例如在原有的多项式展开的基础上，添加傅里叶级数展开、狄利克雷级数展开

